{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2462d890-c1ac-4555-82c2-a91e0123d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3af3fec-e383-4385-884b-7c80ea46b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in each class (Training data):\n",
      "glioma_tumor: 926\n",
      "meningioma_tumor: 937\n",
      "no_tumor: 500\n",
      "pituitary_tumor: 901\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the data\n",
    "data_path = 'data/Unified'\n",
    "\n",
    "# List the subfolders in the training data folder\n",
    "subfolders = os.listdir(data_path)\n",
    "\n",
    "# Initialize a dictionary to hold the count of images in each subfolder\n",
    "image_counts_data = {}\n",
    "\n",
    "# Count the number of images in each subfolder\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(data_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        num_images = len(os.listdir(subfolder_path))\n",
    "        image_counts_data[subfolder] = num_images\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of images in each class (Training data):\")\n",
    "for subfolder, count in image_counts_data.items():\n",
    "    print(f\"{subfolder}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b4ee5d-66c9-4bac-8b97-a859315facb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "original_training_data_path = 'data/Unified'\n",
    "augmented_training_data_path = 'data/Augmented'\n",
    "\n",
    "# Initialize ImageDataGenerator with augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Function to augment images of a specific class\n",
    "def augment_images(class_name, target_count):\n",
    "    class_path = os.path.join(original_training_data_path, class_name)\n",
    "    augmented_class_path = os.path.join(augmented_training_data_path, class_name)\n",
    "    \n",
    "    # Create subfolder in augmented_training if it doesn't exist\n",
    "    if not os.path.exists(augmented_class_path):\n",
    "        os.makedirs(augmented_class_path)\n",
    "    \n",
    "    # List existing images\n",
    "    existing_images = os.listdir(class_path)\n",
    "    existing_count = len(existing_images)\n",
    "    \n",
    "    # Copy existing images to augmented_training\n",
    "    for image_name in existing_images:\n",
    "        src_path = os.path.join(class_path, image_name)\n",
    "        dst_path = os.path.join(augmented_class_path, image_name)\n",
    "        if not os.path.exists(dst_path):  # Check if image already exists in destination\n",
    "            shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    # Start augmentation until target_count is reached\n",
    "    for i in range(target_count - existing_count):\n",
    "        # Randomly select an image to augment\n",
    "        image_name = np.random.choice(existing_images)\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        img = image.load_img(image_path, target_size=(128, 128))\n",
    "        \n",
    "        # Convert to numpy array and add batch dimension\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "        # Generate augmented image\n",
    "        augmented_images = next(datagen.flow(x, batch_size=1))\n",
    "        augmented_image = image.array_to_img(augmented_images[0])\n",
    "        \n",
    "        # Save augmented image\n",
    "        save_path = os.path.join(augmented_class_path, f\"aug_{i}_{image_name}\")\n",
    "        augmented_image.save(save_path)\n",
    "\n",
    "# Function to copy images of a specific class\n",
    "def copy_images(class_name):\n",
    "    class_path = os.path.join(original_training_data_path, class_name)\n",
    "    augmented_class_path = os.path.join(augmented_training_data_path, class_name)\n",
    "    \n",
    "    # Create subfolder in augmented_training if it doesn't exist\n",
    "    if not os.path.exists(augmented_class_path):\n",
    "        os.makedirs(augmented_class_path)\n",
    "    \n",
    "    # List existing images\n",
    "    existing_images = os.listdir(class_path)\n",
    "    \n",
    "    # Copy existing images to augmented_training\n",
    "    for image_name in existing_images:\n",
    "        src_path = os.path.join(class_path, image_name)\n",
    "        dst_path = os.path.join(augmented_class_path, image_name)\n",
    "        if not os.path.exists(dst_path):  # Check if image already exists in destination\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "# Get the number of images in each class in the original training data\n",
    "image_counts = {class_name: len(os.listdir(os.path.join(original_training_data_path, class_name))) \n",
    "                for class_name in os.listdir(original_training_data_path) \n",
    "                if os.path.isdir(os.path.join(original_training_data_path, class_name))}\n",
    "\n",
    "# Find the class with the maximum number of images\n",
    "max_count = max(image_counts.values())\n",
    "\n",
    "# Check if augmentation for 'no_tumor' is needed\n",
    "augmented_no_tumor_path = os.path.join(augmented_training_data_path, 'no_tumor')\n",
    "if not os.path.exists(augmented_no_tumor_path) or len(os.listdir(augmented_no_tumor_path)) < max_count:\n",
    "    # Create augmented_training folder if it doesn't exist\n",
    "    if not os.path.exists(augmented_training_data_path):\n",
    "        os.makedirs(augmented_training_data_path)\n",
    "    \n",
    "    augment_images('no_tumor', max_count)\n",
    "else:\n",
    "    print(\"Augmented images already exist. Skipping augmentation.\")\n",
    "\n",
    "# Copy all original training images to augmented_training\n",
    "for class_name in image_counts.keys():\n",
    "    if class_name != 'no_tumor':  # We've already handled no_tumor\n",
    "        copy_images(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc446df-cbb5-4559-97c2-7aa75451eb4e",
   "metadata": {},
   "source": [
    "## Calculating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cda7e33-f063-4661-b8b2-a5b87847a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {'glioma_tumor': 0.8812095032397408, 'meningioma_tumor': 0.8708644610458911, 'no_tumor': 1.632, 'pituitary_tumor': 0.9056603773584906}\n"
     ]
    }
   ],
   "source": [
    "# Classes\n",
    "classes = list(image_counts.keys())\n",
    "\n",
    "# Number of samples in each class\n",
    "samples_per_class = [image_counts[cls] for cls in classes]\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=[cls for cls in classes for _ in range(image_counts[cls])])\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "print(\"Class Weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39fdaa-8310-46e3-97bc-995feb1dbffc",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12930136-53f3-4007-b082-5de71e795e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "img_height, img_width = 128, 128\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d63cb27-05b0-47d8-8251-c7a1e0c5a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # set the validation split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "498b93a7-2c75-4ba0-9d37-7b52903dd9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2612 images belonging to 4 classes.\n",
      "Found 652 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # set as training data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # set as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274b01ea-f198-4289-8e45-e35c3c46f6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "print(len(train_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aec3b8c-ff49-470b-ad73-122a7c4e5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of images and labels\n",
    "images, labels = train_generator[0]\n",
    "\n",
    "# Get the first image and label from the batch\n",
    "first_image = images[0]\n",
    "first_label = labels[0]\n",
    "\n",
    "# Convert the image array to a Pillow Image\n",
    "first_image = Image.fromarray((first_image * 255).astype('uint8'))\n",
    "\n",
    "# Show the image\n",
    "#first_image.show()\n",
    "\n",
    "# Print the label\n",
    "print(f'Label: {first_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc23d26c-6c9a-4df6-9e4c-a7a1dc846bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (128, 128, 3)\n",
      "Number of Batches per Epoch: 82\n",
      "Total Number of Samples: 2612\n",
      "Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the images\n",
    "image_shape = train_generator.image_shape\n",
    "print(f'Image Shape: {image_shape}')\n",
    "\n",
    "# Get the number of batches per epoch\n",
    "num_batches = len(train_generator)\n",
    "print(f'Number of Batches per Epoch: {num_batches}')\n",
    "\n",
    "# Get the total number of samples\n",
    "num_samples = train_generator.n\n",
    "print(f'Total Number of Samples: {num_samples}')\n",
    "\n",
    "# Get the batch size\n",
    "batch_size = train_generator.batch_size\n",
    "print(f'Batch Size: {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3647118-1119-4e06-a851-2e04713b4ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "sample_image = images[10]\n",
    "print(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23982f9d-3339-4bc5-aaef-d32efd843299",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdde6e92-f2ef-42d4-9367-2138d542fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class indices\n",
    "class_indices = train_generator.class_indices\n",
    "\n",
    "# Map class names to class indices\n",
    "class_weight_indices = {class_indices[class_name]: weight for class_name, weight in class_weight_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9db9ba-b9c9-453e-8136-265086d75a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 22s 256ms/step - loss: 1.5131 - accuracy: 0.3384 - val_loss: 1.3282 - val_accuracy: 0.3113\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 21s 253ms/step - loss: 1.1466 - accuracy: 0.5211 - val_loss: 1.4159 - val_accuracy: 0.3160\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 21s 251ms/step - loss: 1.0834 - accuracy: 0.5475 - val_loss: 1.3355 - val_accuracy: 0.3067\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 21s 253ms/step - loss: 1.0552 - accuracy: 0.5758 - val_loss: 1.3626 - val_accuracy: 0.3635\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 21s 256ms/step - loss: 1.0354 - accuracy: 0.5808 - val_loss: 1.5711 - val_accuracy: 0.3098\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 23s 282ms/step - loss: 1.0080 - accuracy: 0.5911 - val_loss: 1.3125 - val_accuracy: 0.3988\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 21s 251ms/step - loss: 0.9891 - accuracy: 0.6095 - val_loss: 1.3896 - val_accuracy: 0.3957\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 20s 248ms/step - loss: 1.0088 - accuracy: 0.5831 - val_loss: 1.3491 - val_accuracy: 0.3543\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 20s 248ms/step - loss: 0.9604 - accuracy: 0.6164 - val_loss: 1.3312 - val_accuracy: 0.4034\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 20s 248ms/step - loss: 0.9453 - accuracy: 0.6313 - val_loss: 1.2890 - val_accuracy: 0.4218\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the model\n",
    "model_1 = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_1 = model_1.fit(train_generator, epochs=10, validation_data=validation_generator, class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc685d5-6ba9-4f51-a755-2a11234595d4",
   "metadata": {},
   "source": [
    "The accuracy on the train data is much higher than the validation accuracy, so we will try other configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60028313-9b88-4e61-9000-06ef4e97f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 27s 317ms/step - loss: 1.3289 - accuracy: 0.3714 - val_loss: 1.4070 - val_accuracy: 0.2684\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 26s 317ms/step - loss: 1.1220 - accuracy: 0.5268 - val_loss: 1.3995 - val_accuracy: 0.3451\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 26s 316ms/step - loss: 1.0507 - accuracy: 0.5410 - val_loss: 1.3094 - val_accuracy: 0.4018\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 26s 317ms/step - loss: 0.9781 - accuracy: 0.5946 - val_loss: 1.2228 - val_accuracy: 0.4310\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 26s 319ms/step - loss: 0.9382 - accuracy: 0.6126 - val_loss: 1.2138 - val_accuracy: 0.4110\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 26s 316ms/step - loss: 0.9114 - accuracy: 0.6309 - val_loss: 1.4231 - val_accuracy: 0.3926\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 26s 314ms/step - loss: 0.8570 - accuracy: 0.6528 - val_loss: 1.3627 - val_accuracy: 0.4371\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 26s 315ms/step - loss: 0.8654 - accuracy: 0.6359 - val_loss: 1.2822 - val_accuracy: 0.3988\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 26s 314ms/step - loss: 0.8302 - accuracy: 0.6600 - val_loss: 1.3213 - val_accuracy: 0.4095\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 26s 315ms/step - loss: 0.8297 - accuracy: 0.6596 - val_loss: 1.4235 - val_accuracy: 0.4356\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_2 = keras.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # Second convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history_2 = model_2.fit(train_generator, epochs=10, validation_data=validation_generator, class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c48e5867-ad5d-4a86-9476-fce31da48ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 30s 343ms/step - loss: 5.1270 - accuracy: 0.3959 - val_loss: 5.3786 - val_accuracy: 0.2822\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 28s 337ms/step - loss: 5.1139 - accuracy: 0.4192 - val_loss: 16.5012 - val_accuracy: 0.2761\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 27s 330ms/step - loss: 5.1155 - accuracy: 0.4495 - val_loss: 21.8400 - val_accuracy: 0.2761\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 28s 338ms/step - loss: 5.0139 - accuracy: 0.4701 - val_loss: 19.9736 - val_accuracy: 0.2914\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "# Data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model_3 = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_3 = model_3.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weight_indices,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd291643-c1ab-4280-9070-d2140c972c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 30s 340ms/step - loss: 1.9315 - accuracy: 0.4116 - val_loss: 1.8548 - val_accuracy: 0.2761\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 28s 337ms/step - loss: 1.8016 - accuracy: 0.4453 - val_loss: 3.5363 - val_accuracy: 0.2761\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 28s 341ms/step - loss: 1.7827 - accuracy: 0.4472 - val_loss: 5.0251 - val_accuracy: 0.2761\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 28s 342ms/step - loss: 1.7246 - accuracy: 0.4709 - val_loss: 6.2580 - val_accuracy: 0.2776\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 27s 329ms/step - loss: 1.6619 - accuracy: 0.4908 - val_loss: 5.6708 - val_accuracy: 0.2853\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 27s 325ms/step - loss: 1.6726 - accuracy: 0.4954 - val_loss: 5.4648 - val_accuracy: 0.3006\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 26s 320ms/step - loss: 1.6190 - accuracy: 0.5096 - val_loss: 3.9367 - val_accuracy: 0.3328\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 26s 319ms/step - loss: 1.5662 - accuracy: 0.5218 - val_loss: 4.7414 - val_accuracy: 0.3604\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 27s 330ms/step - loss: 1.5437 - accuracy: 0.5218 - val_loss: 3.1145 - val_accuracy: 0.3742\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 27s 334ms/step - loss: 1.5500 - accuracy: 0.5314 - val_loss: 3.3255 - val_accuracy: 0.3727\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model_4 = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_4.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_4 = model_4.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weight_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731fa25-8406-4dec-bfdf-e79a2240f971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fdaba-e2b5-4627-a4ff-eb6d81c5a3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f363d-1d88-45dc-8b33-551c0e60997d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f59e9b-3c59-460f-877c-bd553e75f050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf1ce2-1c0e-4dd2-878f-4d55f2c72d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c51f6e-04ea-475a-9975-083722d704de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d68cd-923c-45e3-9352-edb0bd38b428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a02247-e306-4879-8943-5bd3f8e24b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1ab7a-bed4-4fba-85b9-0d1cf1782dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ea5de-98fa-4cc1-8100-52d59adc9ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3045ccc-ae32-4a8b-a9fe-4e3263ab91c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "82/82 [==============================] - 29s 315ms/step - loss: 1.5034 - mae: 0.3571 - accuracy: 0.3101 - val_loss: 1.2718 - val_mae: 0.3348 - val_accuracy: 0.4141\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 25s 300ms/step - loss: 1.2683 - mae: 0.3279 - accuracy: 0.4230 - val_loss: 1.1989 - val_mae: 0.3200 - val_accuracy: 0.4908\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 25s 306ms/step - loss: 1.1282 - mae: 0.3030 - accuracy: 0.5023 - val_loss: 1.1570 - val_mae: 0.3098 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 25s 304ms/step - loss: 1.0186 - mae: 0.2819 - accuracy: 0.5593 - val_loss: 1.0657 - val_mae: 0.2955 - val_accuracy: 0.5337\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 25s 301ms/step - loss: 0.9590 - mae: 0.2684 - accuracy: 0.5992 - val_loss: 1.0252 - val_mae: 0.2842 - val_accuracy: 0.5767\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 25s 303ms/step - loss: 0.9157 - mae: 0.2553 - accuracy: 0.6283 - val_loss: 1.0269 - val_mae: 0.2776 - val_accuracy: 0.5583\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 25s 303ms/step - loss: 0.8491 - mae: 0.2415 - accuracy: 0.6665 - val_loss: 1.0256 - val_mae: 0.2736 - val_accuracy: 0.5844\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 25s 306ms/step - loss: 0.8271 - mae: 0.2338 - accuracy: 0.6769 - val_loss: 1.0178 - val_mae: 0.2671 - val_accuracy: 0.5782\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 25s 301ms/step - loss: 0.8048 - mae: 0.2279 - accuracy: 0.6799 - val_loss: 0.9754 - val_mae: 0.2651 - val_accuracy: 0.5905\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 25s 301ms/step - loss: 0.7771 - mae: 0.2223 - accuracy: 0.6880 - val_loss: 0.9424 - val_mae: 0.2582 - val_accuracy: 0.5997\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 25s 301ms/step - loss: 0.7499 - mae: 0.2150 - accuracy: 0.7083 - val_loss: 0.9757 - val_mae: 0.2568 - val_accuracy: 0.6104\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 25s 299ms/step - loss: 0.7329 - mae: 0.2102 - accuracy: 0.7113 - val_loss: 0.9737 - val_mae: 0.2545 - val_accuracy: 0.6120\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 25s 303ms/step - loss: 0.7181 - mae: 0.2077 - accuracy: 0.7175 - val_loss: 0.9329 - val_mae: 0.2491 - val_accuracy: 0.6212\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 25s 304ms/step - loss: 0.7126 - mae: 0.2046 - accuracy: 0.7221 - val_loss: 0.9101 - val_mae: 0.2458 - val_accuracy: 0.6212\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 25s 306ms/step - loss: 0.6955 - mae: 0.2012 - accuracy: 0.7236 - val_loss: 0.9278 - val_mae: 0.2414 - val_accuracy: 0.6426\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 25s 304ms/step - loss: 0.6878 - mae: 0.1967 - accuracy: 0.7377 - val_loss: 0.8691 - val_mae: 0.2309 - val_accuracy: 0.6718\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 25s 305ms/step - loss: 0.6781 - mae: 0.1947 - accuracy: 0.7389 - val_loss: 0.9293 - val_mae: 0.2402 - val_accuracy: 0.6258\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 25s 300ms/step - loss: 0.6642 - mae: 0.1907 - accuracy: 0.7443 - val_loss: 0.9087 - val_mae: 0.2387 - val_accuracy: 0.6426\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 25s 300ms/step - loss: 0.6815 - mae: 0.1930 - accuracy: 0.7389 - val_loss: 0.9269 - val_mae: 0.2361 - val_accuracy: 0.6365\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 24s 298ms/step - loss: 0.6550 - mae: 0.1870 - accuracy: 0.7462 - val_loss: 0.8912 - val_mae: 0.2320 - val_accuracy: 0.6442\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load MobileNetV2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(128,128,3), \n",
    "                                               include_top=False, \n",
    "                                               weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model with custom layers\n",
    "model_5 = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with a smaller learning rate\n",
    "model_5.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['mae', 'accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "history_5 = model_5.fit(train_generator, \n",
    "                        epochs=20, \n",
    "                        validation_data=validation_generator, \n",
    "                        class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5036c133-1284-42f7-9f44-b894eb7b86dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "82/82 [==============================] - 21s 224ms/step - loss: 1.4684 - mae: 0.3410 - accuracy: 0.3802 - precision_10: 0.4157 - recall_10: 0.2247 - val_loss: 2.0448 - val_mae: 0.3586 - val_accuracy: 0.3083 - val_precision_10: 0.3528 - val_recall_10: 0.2224\n",
      "Epoch 2/15\n",
      "82/82 [==============================] - 21s 260ms/step - loss: 1.0746 - mae: 0.2844 - accuracy: 0.5593 - precision_10: 0.6484 - recall_10: 0.4024 - val_loss: 1.7571 - val_mae: 0.3264 - val_accuracy: 0.4279 - val_precision_10: 0.5101 - val_recall_10: 0.3497\n",
      "Epoch 3/15\n",
      "82/82 [==============================] - 26s 312ms/step - loss: 0.9053 - mae: 0.2481 - accuracy: 0.6394 - precision_10: 0.7188 - recall_10: 0.5119 - val_loss: 1.5660 - val_mae: 0.3078 - val_accuracy: 0.4509 - val_precision_10: 0.5309 - val_recall_10: 0.3819\n",
      "Epoch 4/15\n",
      "82/82 [==============================] - 26s 312ms/step - loss: 0.8158 - mae: 0.2260 - accuracy: 0.6838 - precision_10: 0.7535 - recall_10: 0.5792 - val_loss: 1.3624 - val_mae: 0.2884 - val_accuracy: 0.4954 - val_precision_10: 0.5503 - val_recall_10: 0.3942\n",
      "Epoch 5/15\n",
      "82/82 [==============================] - 26s 320ms/step - loss: 0.7508 - mae: 0.2082 - accuracy: 0.7132 - precision_10: 0.7870 - recall_10: 0.6294 - val_loss: 1.2308 - val_mae: 0.2708 - val_accuracy: 0.5506 - val_precision_10: 0.6189 - val_recall_10: 0.4509\n",
      "Epoch 6/15\n",
      "82/82 [==============================] - 26s 317ms/step - loss: 0.7119 - mae: 0.1988 - accuracy: 0.7332 - precision_10: 0.7868 - recall_10: 0.6585 - val_loss: 1.1599 - val_mae: 0.2610 - val_accuracy: 0.5552 - val_precision_10: 0.6283 - val_recall_10: 0.4770\n",
      "Epoch 7/15\n",
      "82/82 [==============================] - 27s 325ms/step - loss: 0.6521 - mae: 0.1876 - accuracy: 0.7496 - precision_10: 0.8064 - recall_10: 0.6776 - val_loss: 1.0657 - val_mae: 0.2502 - val_accuracy: 0.5798 - val_precision_10: 0.6579 - val_recall_10: 0.4985\n",
      "Epoch 8/15\n",
      "82/82 [==============================] - 26s 316ms/step - loss: 0.6204 - mae: 0.1781 - accuracy: 0.7611 - precision_10: 0.8189 - recall_10: 0.6995 - val_loss: 0.9523 - val_mae: 0.2339 - val_accuracy: 0.6104 - val_precision_10: 0.6881 - val_recall_10: 0.5245\n",
      "Epoch 9/15\n",
      "82/82 [==============================] - 26s 314ms/step - loss: 0.5916 - mae: 0.1719 - accuracy: 0.7691 - precision_10: 0.8167 - recall_10: 0.7132 - val_loss: 0.9148 - val_mae: 0.2291 - val_accuracy: 0.6442 - val_precision_10: 0.7337 - val_recall_10: 0.5706\n",
      "Epoch 10/15\n",
      "82/82 [==============================] - 26s 312ms/step - loss: 0.5842 - mae: 0.1666 - accuracy: 0.7749 - precision_10: 0.8179 - recall_10: 0.7190 - val_loss: 0.8812 - val_mae: 0.2223 - val_accuracy: 0.6595 - val_precision_10: 0.7068 - val_recall_10: 0.5583\n",
      "Epoch 11/15\n",
      "82/82 [==============================] - 26s 322ms/step - loss: 0.5677 - mae: 0.1634 - accuracy: 0.7845 - precision_10: 0.8276 - recall_10: 0.7316 - val_loss: 0.8184 - val_mae: 0.2117 - val_accuracy: 0.6702 - val_precision_10: 0.7476 - val_recall_10: 0.6089\n",
      "Epoch 12/15\n",
      "82/82 [==============================] - 26s 323ms/step - loss: 0.5325 - mae: 0.1555 - accuracy: 0.7948 - precision_10: 0.8391 - recall_10: 0.7466 - val_loss: 0.7769 - val_mae: 0.2051 - val_accuracy: 0.6994 - val_precision_10: 0.7435 - val_recall_10: 0.6181\n",
      "Epoch 13/15\n",
      "82/82 [==============================] - 26s 319ms/step - loss: 0.5640 - mae: 0.1585 - accuracy: 0.7810 - precision_10: 0.8194 - recall_10: 0.7297 - val_loss: 0.7484 - val_mae: 0.1987 - val_accuracy: 0.6933 - val_precision_10: 0.7729 - val_recall_10: 0.6212\n",
      "Epoch 14/15\n",
      "82/82 [==============================] - 26s 321ms/step - loss: 0.5261 - mae: 0.1530 - accuracy: 0.7948 - precision_10: 0.8353 - recall_10: 0.7454 - val_loss: 0.7628 - val_mae: 0.1989 - val_accuracy: 0.7117 - val_precision_10: 0.7694 - val_recall_10: 0.6242\n",
      "Epoch 15/15\n",
      "82/82 [==============================] - 27s 323ms/step - loss: 0.4972 - mae: 0.1485 - accuracy: 0.8032 - precision_10: 0.8356 - recall_10: 0.7611 - val_loss: 0.7208 - val_mae: 0.1905 - val_accuracy: 0.7377 - val_precision_10: 0.7902 - val_recall_10: 0.6702\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load MobileNetV2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(128,128,3), \n",
    "                                               include_top=False, \n",
    "                                               weights=\"imagenet\")\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except for the last 10\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model with custom layers\n",
    "model_6 = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with a smaller learning rate\n",
    "model_6.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=[\n",
    "                    'mae', \n",
    "                    'accuracy', \n",
    "                    tf.keras.metrics.Precision(), \n",
    "                    tf.keras.metrics.Recall()\n",
    "                ])\n",
    "\n",
    "# Continue training\n",
    "history_6 = model_6.fit(train_generator, \n",
    "                             epochs=15, \n",
    "                             validation_data=validation_generator, \n",
    "                             class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e70ed-2da3-4ed3-89ed-0b128257c3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
