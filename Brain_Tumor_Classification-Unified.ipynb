{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2462d890-c1ac-4555-82c2-a91e0123d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3af3fec-e383-4385-884b-7c80ea46b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in each class (Training data):\n",
      "glioma_tumor: 926\n",
      "meningioma_tumor: 937\n",
      "no_tumor: 500\n",
      "pituitary_tumor: 901\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the data\n",
    "data_path = 'data/Unified'\n",
    "\n",
    "# List the subfolders in the training data folder\n",
    "subfolders = os.listdir(data_path)\n",
    "\n",
    "# Initialize a dictionary to hold the count of images in each subfolder\n",
    "image_counts_data = {}\n",
    "\n",
    "# Count the number of images in each subfolder\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(data_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        num_images = len(os.listdir(subfolder_path))\n",
    "        image_counts_data[subfolder] = num_images\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of images in each class:\")\n",
    "for subfolder, count in image_counts_data.items():\n",
    "    print(f\"{subfolder}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b4ee5d-66c9-4bac-8b97-a859315facb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented images already exist. Skipping augmentation.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "original_training_data_path = 'data/Unified'\n",
    "augmented_training_data_path = 'data/Augmented'\n",
    "\n",
    "# Initialize ImageDataGenerator with augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Function to augment images of a specific class\n",
    "def augment_images(class_name, target_count):\n",
    "    class_path = os.path.join(original_training_data_path, class_name)\n",
    "    augmented_class_path = os.path.join(augmented_training_data_path, class_name)\n",
    "    \n",
    "    # Create subfolder in augmented_training if it doesn't exist\n",
    "    if not os.path.exists(augmented_class_path):\n",
    "        os.makedirs(augmented_class_path)\n",
    "    \n",
    "    # List existing images\n",
    "    existing_images = os.listdir(class_path)\n",
    "    existing_count = len(existing_images)\n",
    "    \n",
    "    # Copy existing images to augmented_training\n",
    "    for image_name in existing_images:\n",
    "        src_path = os.path.join(class_path, image_name)\n",
    "        dst_path = os.path.join(augmented_class_path, image_name)\n",
    "        if not os.path.exists(dst_path):  # Check if image already exists in destination\n",
    "            shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    # Start augmentation until target_count is reached\n",
    "    for i in range(target_count - existing_count):\n",
    "        # Randomly select an image to augment\n",
    "        image_name = np.random.choice(existing_images)\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        img = image.load_img(image_path, target_size=(128, 128))\n",
    "        \n",
    "        # Convert to numpy array and add batch dimension\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "        # Generate augmented image\n",
    "        augmented_images = next(datagen.flow(x, batch_size=1))\n",
    "        augmented_image = image.array_to_img(augmented_images[0])\n",
    "        \n",
    "        # Save augmented image\n",
    "        save_path = os.path.join(augmented_class_path, f\"aug_{i}_{image_name}\")\n",
    "        augmented_image.save(save_path)\n",
    "\n",
    "# Function to copy images of a specific class\n",
    "def copy_images(class_name):\n",
    "    class_path = os.path.join(original_training_data_path, class_name)\n",
    "    augmented_class_path = os.path.join(augmented_training_data_path, class_name)\n",
    "    \n",
    "    # Create subfolder in augmented_training if it doesn't exist\n",
    "    if not os.path.exists(augmented_class_path):\n",
    "        os.makedirs(augmented_class_path)\n",
    "    \n",
    "    # List existing images\n",
    "    existing_images = os.listdir(class_path)\n",
    "    \n",
    "    # Copy existing images to augmented_training\n",
    "    for image_name in existing_images:\n",
    "        src_path = os.path.join(class_path, image_name)\n",
    "        dst_path = os.path.join(augmented_class_path, image_name)\n",
    "        if not os.path.exists(dst_path):  # Check if image already exists in destination\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "# Get the number of images in each class in the original training data\n",
    "image_counts = {class_name: len(os.listdir(os.path.join(original_training_data_path, class_name))) \n",
    "                for class_name in os.listdir(original_training_data_path) \n",
    "                if os.path.isdir(os.path.join(original_training_data_path, class_name))}\n",
    "\n",
    "# Find the class with the maximum number of images\n",
    "max_count = max(image_counts.values())\n",
    "\n",
    "# Check if augmentation for 'no_tumor' is needed\n",
    "augmented_no_tumor_path = os.path.join(augmented_training_data_path, 'no_tumor')\n",
    "if not os.path.exists(augmented_no_tumor_path) or len(os.listdir(augmented_no_tumor_path)) < max_count:\n",
    "    # Create augmented_training folder if it doesn't exist\n",
    "    if not os.path.exists(augmented_training_data_path):\n",
    "        os.makedirs(augmented_training_data_path)\n",
    "    \n",
    "    augment_images('no_tumor', max_count)\n",
    "else:\n",
    "    print(\"Augmented images already exist. Skipping augmentation.\")\n",
    "\n",
    "# Copy all original training images to augmented_training\n",
    "for class_name in image_counts.keys():\n",
    "    if class_name != 'no_tumor':  # We've already handled no_tumor\n",
    "        copy_images(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89829636-fce8-477c-9d1d-965f16f800b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in each class after data augmentation:\n",
      "glioma_tumor: 926\n",
      "meningioma_tumor: 937\n",
      "no_tumor: 937\n",
      "pituitary_tumor: 901\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the data\n",
    "data_path = 'data/Augmented'\n",
    "\n",
    "# List the subfolders in the training data folder\n",
    "subfolders = os.listdir(data_path)\n",
    "\n",
    "# Initialize a dictionary to hold the count of images in each subfolder\n",
    "image_counts_data = {}\n",
    "\n",
    "# Count the number of images in each subfolder\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(data_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        num_images = len(os.listdir(subfolder_path))\n",
    "        image_counts_data[subfolder] = num_images\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of images in each class after data augmentation:\")\n",
    "for subfolder, count in image_counts_data.items():\n",
    "    print(f\"{subfolder}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc446df-cbb5-4559-97c2-7aa75451eb4e",
   "metadata": {},
   "source": [
    "## Calculating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cda7e33-f063-4661-b8b2-a5b87847a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {'glioma_tumor': 0.8812095032397408, 'meningioma_tumor': 0.8708644610458911, 'no_tumor': 1.632, 'pituitary_tumor': 0.9056603773584906}\n"
     ]
    }
   ],
   "source": [
    "# Classes\n",
    "classes = list(image_counts.keys())\n",
    "\n",
    "# Number of samples in each class\n",
    "samples_per_class = [image_counts[cls] for cls in classes]\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=[cls for cls in classes for _ in range(image_counts[cls])])\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "print(\"Class Weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39fdaa-8310-46e3-97bc-995feb1dbffc",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12930136-53f3-4007-b082-5de71e795e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "img_height, img_width = 128, 128\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d63cb27-05b0-47d8-8251-c7a1e0c5a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # set the validation split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498b93a7-2c75-4ba0-9d37-7b52903dd9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2612 images belonging to 4 classes.\n",
      "Found 652 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # set as training data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # set as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274b01ea-f198-4289-8e45-e35c3c46f6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(train_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aec3b8c-ff49-470b-ad73-122a7c4e5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of images and labels\n",
    "images, labels = train_generator[0]\n",
    "\n",
    "# Get the first image and label from the batch\n",
    "first_image = images[0]\n",
    "first_label = labels[0]\n",
    "\n",
    "# Convert the image array to a Pillow Image\n",
    "first_image = Image.fromarray((first_image * 255).astype('uint8'))\n",
    "\n",
    "# Show the image\n",
    "#first_image.show()\n",
    "\n",
    "# Print the label\n",
    "print(f'Label: {first_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc23d26c-6c9a-4df6-9e4c-a7a1dc846bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (128, 128, 3)\n",
      "Number of Batches per Epoch: 82\n",
      "Total Number of Samples: 2612\n",
      "Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the images\n",
    "image_shape = train_generator.image_shape\n",
    "print(f'Image Shape: {image_shape}')\n",
    "\n",
    "# Get the number of batches per epoch\n",
    "num_batches = len(train_generator)\n",
    "print(f'Number of Batches per Epoch: {num_batches}')\n",
    "\n",
    "# Get the total number of samples\n",
    "num_samples = train_generator.n\n",
    "print(f'Total Number of Samples: {num_samples}')\n",
    "\n",
    "# Get the batch size\n",
    "batch_size = train_generator.batch_size\n",
    "print(f'Batch Size: {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3647118-1119-4e06-a851-2e04713b4ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.00113512 0.00113512 0.00113512]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.00431758 0.00431758 0.00431758]\n",
      "  [0.00266152 0.00266152 0.00266152]\n",
      "  [0.00120347 0.00120347 0.00120347]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.01028649 0.01028649 0.01028649]\n",
      "  [0.00737039 0.00737039 0.00737039]\n",
      "  [0.00463892 0.00463892 0.00463892]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "sample_image = images[10]\n",
    "print(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23982f9d-3339-4bc5-aaef-d32efd843299",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdde6e92-f2ef-42d4-9367-2138d542fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class indices\n",
    "class_indices = train_generator.class_indices\n",
    "\n",
    "# Map class names to class indices\n",
    "class_weight_indices = {class_indices[class_name]: weight for class_name, weight in class_weight_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9db9ba-b9c9-453e-8136-265086d75a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 25s 299ms/step - loss: 1.4295 - accuracy: 0.3637 - val_loss: 1.3835 - val_accuracy: 0.3190\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 21s 252ms/step - loss: 1.1870 - accuracy: 0.4870 - val_loss: 1.2459 - val_accuracy: 0.3788\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 21s 251ms/step - loss: 1.1149 - accuracy: 0.5333 - val_loss: 1.2986 - val_accuracy: 0.3604\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 21s 257ms/step - loss: 1.0933 - accuracy: 0.5513 - val_loss: 1.3011 - val_accuracy: 0.4248\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 21s 257ms/step - loss: 1.0696 - accuracy: 0.5647 - val_loss: 1.2309 - val_accuracy: 0.4233\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 21s 255ms/step - loss: 1.0272 - accuracy: 0.5846 - val_loss: 1.5588 - val_accuracy: 0.3236\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 21s 260ms/step - loss: 1.0291 - accuracy: 0.5884 - val_loss: 1.2641 - val_accuracy: 0.4049\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 22s 270ms/step - loss: 1.0196 - accuracy: 0.5869 - val_loss: 1.2897 - val_accuracy: 0.4156\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 21s 255ms/step - loss: 1.0034 - accuracy: 0.5949 - val_loss: 1.2525 - val_accuracy: 0.4494\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 21s 261ms/step - loss: 0.9799 - accuracy: 0.6172 - val_loss: 1.3697 - val_accuracy: 0.4494\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the model\n",
    "model_1 = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_1 = model_1.fit(train_generator, epochs=10, validation_data=validation_generator, class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc685d5-6ba9-4f51-a755-2a11234595d4",
   "metadata": {},
   "source": [
    "The accuracy on the train data is much higher than the validation accuracy, so we will try other configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60028313-9b88-4e61-9000-06ef4e97f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 28s 335ms/step - loss: 1.3644 - accuracy: 0.3250 - val_loss: 1.4796 - val_accuracy: 0.2531\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 26s 322ms/step - loss: 1.1454 - accuracy: 0.4958 - val_loss: 1.3095 - val_accuracy: 0.4080\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 27s 326ms/step - loss: 1.0419 - accuracy: 0.5701 - val_loss: 1.3390 - val_accuracy: 0.3834\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 27s 326ms/step - loss: 0.9622 - accuracy: 0.6026 - val_loss: 1.2222 - val_accuracy: 0.4141\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 27s 325ms/step - loss: 0.9452 - accuracy: 0.6141 - val_loss: 1.3156 - val_accuracy: 0.4202\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 27s 327ms/step - loss: 0.8944 - accuracy: 0.6340 - val_loss: 1.2543 - val_accuracy: 0.4248\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 26s 317ms/step - loss: 0.8895 - accuracy: 0.6390 - val_loss: 1.3458 - val_accuracy: 0.4202\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 27s 323ms/step - loss: 0.8429 - accuracy: 0.6577 - val_loss: 1.3013 - val_accuracy: 0.4233\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 26s 322ms/step - loss: 0.8073 - accuracy: 0.6650 - val_loss: 1.5439 - val_accuracy: 0.3926\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 27s 325ms/step - loss: 0.8036 - accuracy: 0.6665 - val_loss: 1.4737 - val_accuracy: 0.3758\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_2 = keras.Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    # Second convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history_2 = model_2.fit(train_generator, epochs=10, validation_data=validation_generator, class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c48e5867-ad5d-4a86-9476-fce31da48ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 30s 344ms/step - loss: 5.1008 - accuracy: 0.3748 - val_loss: 7.1322 - val_accuracy: 0.2761\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 28s 342ms/step - loss: 6.0232 - accuracy: 0.4146 - val_loss: 17.3816 - val_accuracy: 0.2761\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 28s 339ms/step - loss: 4.9953 - accuracy: 0.4541 - val_loss: 19.0342 - val_accuracy: 0.2761\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 28s 338ms/step - loss: 4.4461 - accuracy: 0.4801 - val_loss: 19.8111 - val_accuracy: 0.2776\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "# Data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model_3 = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_3 = model_3.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weight_indices,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd291643-c1ab-4280-9070-d2140c972c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 30s 340ms/step - loss: 1.9265 - accuracy: 0.4089 - val_loss: 1.7362 - val_accuracy: 0.2761\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 28s 340ms/step - loss: 1.8100 - accuracy: 0.4414 - val_loss: 2.7542 - val_accuracy: 0.2761\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 28s 337ms/step - loss: 1.7643 - accuracy: 0.4602 - val_loss: 4.0133 - val_accuracy: 0.2761\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 28s 339ms/step - loss: 1.7579 - accuracy: 0.4713 - val_loss: 4.1010 - val_accuracy: 0.2791\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 28s 341ms/step - loss: 1.7180 - accuracy: 0.4832 - val_loss: 3.7306 - val_accuracy: 0.3098\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 28s 340ms/step - loss: 1.5737 - accuracy: 0.5054 - val_loss: 2.6407 - val_accuracy: 0.3298\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 28s 334ms/step - loss: 1.6311 - accuracy: 0.5203 - val_loss: 2.3437 - val_accuracy: 0.3574\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 27s 335ms/step - loss: 1.5798 - accuracy: 0.5077 - val_loss: 2.1971 - val_accuracy: 0.3942\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 28s 337ms/step - loss: 1.4954 - accuracy: 0.5352 - val_loss: 2.3275 - val_accuracy: 0.3819\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 28s 336ms/step - loss: 1.5374 - accuracy: 0.5276 - val_loss: 2.1909 - val_accuracy: 0.4018\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model_4 = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_4.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_4 = model_4.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weight_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3045ccc-ae32-4a8b-a9fe-4e3263ab91c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "82/82 [==============================] - 30s 334ms/step - loss: 1.4705 - mae: 0.3543 - accuracy: 0.3281 - val_loss: 1.3084 - val_mae: 0.3412 - val_accuracy: 0.3850\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 25s 309ms/step - loss: 1.2526 - mae: 0.3264 - accuracy: 0.4273 - val_loss: 1.2191 - val_mae: 0.3257 - val_accuracy: 0.4310\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 1.0912 - mae: 0.2981 - accuracy: 0.5241 - val_loss: 1.0943 - val_mae: 0.2988 - val_accuracy: 0.5506\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 24s 287ms/step - loss: 1.0369 - mae: 0.2837 - accuracy: 0.5651 - val_loss: 1.1274 - val_mae: 0.3020 - val_accuracy: 0.5169\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 24s 293ms/step - loss: 0.9506 - mae: 0.2656 - accuracy: 0.6018 - val_loss: 1.0563 - val_mae: 0.2889 - val_accuracy: 0.5567\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.9035 - mae: 0.2534 - accuracy: 0.6378 - val_loss: 1.0379 - val_mae: 0.2817 - val_accuracy: 0.5567\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.8695 - mae: 0.2471 - accuracy: 0.6462 - val_loss: 0.9969 - val_mae: 0.2709 - val_accuracy: 0.5905\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.8288 - mae: 0.2368 - accuracy: 0.6723 - val_loss: 0.9955 - val_mae: 0.2692 - val_accuracy: 0.6012\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.7969 - mae: 0.2290 - accuracy: 0.6769 - val_loss: 0.9646 - val_mae: 0.2595 - val_accuracy: 0.6104\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.7704 - mae: 0.2208 - accuracy: 0.7006 - val_loss: 0.9756 - val_mae: 0.2600 - val_accuracy: 0.6089\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 24s 292ms/step - loss: 0.7690 - mae: 0.2188 - accuracy: 0.7064 - val_loss: 0.9424 - val_mae: 0.2529 - val_accuracy: 0.5874\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.7348 - mae: 0.2117 - accuracy: 0.7025 - val_loss: 0.9245 - val_mae: 0.2486 - val_accuracy: 0.6304\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.7241 - mae: 0.2087 - accuracy: 0.7224 - val_loss: 0.9260 - val_mae: 0.2449 - val_accuracy: 0.6196\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.7061 - mae: 0.2034 - accuracy: 0.7270 - val_loss: 0.9125 - val_mae: 0.2449 - val_accuracy: 0.6150\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 24s 290ms/step - loss: 0.7010 - mae: 0.2012 - accuracy: 0.7255 - val_loss: 0.9185 - val_mae: 0.2423 - val_accuracy: 0.6288\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.6784 - mae: 0.1963 - accuracy: 0.7393 - val_loss: 0.9339 - val_mae: 0.2433 - val_accuracy: 0.6212\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 24s 288ms/step - loss: 0.6661 - mae: 0.1936 - accuracy: 0.7481 - val_loss: 0.9350 - val_mae: 0.2399 - val_accuracy: 0.6396\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 24s 291ms/step - loss: 0.6614 - mae: 0.1900 - accuracy: 0.7511 - val_loss: 0.9388 - val_mae: 0.2364 - val_accuracy: 0.6411\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 24s 293ms/step - loss: 0.6605 - mae: 0.1891 - accuracy: 0.7435 - val_loss: 0.9204 - val_mae: 0.2368 - val_accuracy: 0.6549\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 24s 289ms/step - loss: 0.6350 - mae: 0.1855 - accuracy: 0.7492 - val_loss: 0.9063 - val_mae: 0.2365 - val_accuracy: 0.6288\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load MobileNetV2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(128,128,3), \n",
    "                                               include_top=False, \n",
    "                                               weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model with custom layers\n",
    "model_5 = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with a smaller learning rate\n",
    "model_5.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['mae', 'accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "history_5 = model_5.fit(train_generator, \n",
    "                        epochs=20, \n",
    "                        validation_data=validation_generator, \n",
    "                        class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5036c133-1284-42f7-9f44-b894eb7b86dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "82/82 [==============================] - 18s 195ms/step - loss: 1.6130 - mae: 0.3585 - accuracy: 0.3151 - precision_7: 0.3210 - recall_7: 0.1593 - val_loss: 1.6322 - val_mae: 0.3535 - val_accuracy: 0.3374 - val_precision_7: 0.3520 - val_recall_7: 0.2117\n",
      "Epoch 2/15\n",
      "82/82 [==============================] - 15s 184ms/step - loss: 1.1748 - mae: 0.3050 - accuracy: 0.4858 - precision_7: 0.5742 - recall_7: 0.3185 - val_loss: 1.4898 - val_mae: 0.3326 - val_accuracy: 0.3558 - val_precision_7: 0.4005 - val_recall_7: 0.2684\n",
      "Epoch 3/15\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.9622 - mae: 0.2636 - accuracy: 0.6095 - precision_7: 0.6960 - recall_7: 0.4609 - val_loss: 1.3965 - val_mae: 0.3131 - val_accuracy: 0.4187 - val_precision_7: 0.4426 - val_recall_7: 0.3313\n",
      "Epoch 4/15\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.8397 - mae: 0.2347 - accuracy: 0.6646 - precision_7: 0.7496 - recall_7: 0.5582 - val_loss: 1.2766 - val_mae: 0.2950 - val_accuracy: 0.4663 - val_precision_7: 0.4978 - val_recall_7: 0.3543\n",
      "Epoch 5/15\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.7667 - mae: 0.2166 - accuracy: 0.6956 - precision_7: 0.7671 - recall_7: 0.6003 - val_loss: 1.1912 - val_mae: 0.2846 - val_accuracy: 0.4693 - val_precision_7: 0.5190 - val_recall_7: 0.3773\n",
      "Epoch 6/15\n",
      "82/82 [==============================] - 15s 187ms/step - loss: 0.7218 - mae: 0.2032 - accuracy: 0.7121 - precision_7: 0.7764 - recall_7: 0.6355 - val_loss: 1.0818 - val_mae: 0.2674 - val_accuracy: 0.5307 - val_precision_7: 0.5890 - val_recall_7: 0.4417\n",
      "Epoch 7/15\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.6845 - mae: 0.1939 - accuracy: 0.7316 - precision_7: 0.7919 - recall_7: 0.6543 - val_loss: 1.0401 - val_mae: 0.2600 - val_accuracy: 0.5460 - val_precision_7: 0.5748 - val_recall_7: 0.4479\n",
      "Epoch 8/15\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.6478 - mae: 0.1850 - accuracy: 0.7431 - precision_7: 0.7968 - recall_7: 0.6742 - val_loss: 0.9936 - val_mae: 0.2439 - val_accuracy: 0.5920 - val_precision_7: 0.6600 - val_recall_7: 0.5031\n",
      "Epoch 9/15\n",
      "82/82 [==============================] - 16s 190ms/step - loss: 0.6142 - mae: 0.1769 - accuracy: 0.7573 - precision_7: 0.8133 - recall_7: 0.7002 - val_loss: 0.9505 - val_mae: 0.2363 - val_accuracy: 0.6150 - val_precision_7: 0.6738 - val_recall_7: 0.5322\n",
      "Epoch 10/15\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.5798 - mae: 0.1685 - accuracy: 0.7703 - precision_7: 0.8177 - recall_7: 0.7159 - val_loss: 0.9258 - val_mae: 0.2325 - val_accuracy: 0.6227 - val_precision_7: 0.6769 - val_recall_7: 0.5399\n",
      "Epoch 11/15\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.5772 - mae: 0.1659 - accuracy: 0.7703 - precision_7: 0.8153 - recall_7: 0.7182 - val_loss: 0.8625 - val_mae: 0.2204 - val_accuracy: 0.6442 - val_precision_7: 0.6937 - val_recall_7: 0.5767\n",
      "Epoch 12/15\n",
      "82/82 [==============================] - 15s 187ms/step - loss: 0.5528 - mae: 0.1597 - accuracy: 0.7860 - precision_7: 0.8327 - recall_7: 0.7355 - val_loss: 0.8187 - val_mae: 0.2136 - val_accuracy: 0.6518 - val_precision_7: 0.7091 - val_recall_7: 0.5982\n",
      "Epoch 13/15\n",
      "82/82 [==============================] - 15s 186ms/step - loss: 0.5458 - mae: 0.1579 - accuracy: 0.7917 - precision_7: 0.8317 - recall_7: 0.7397 - val_loss: 0.8154 - val_mae: 0.2067 - val_accuracy: 0.6856 - val_precision_7: 0.7597 - val_recall_7: 0.6304\n",
      "Epoch 14/15\n",
      "82/82 [==============================] - 15s 187ms/step - loss: 0.5351 - mae: 0.1555 - accuracy: 0.7925 - precision_7: 0.8366 - recall_7: 0.7450 - val_loss: 0.8013 - val_mae: 0.2019 - val_accuracy: 0.7040 - val_precision_7: 0.7382 - val_recall_7: 0.6227\n",
      "Epoch 15/15\n",
      "82/82 [==============================] - 15s 187ms/step - loss: 0.5230 - mae: 0.1504 - accuracy: 0.7925 - precision_7: 0.8338 - recall_7: 0.7435 - val_loss: 0.7831 - val_mae: 0.1963 - val_accuracy: 0.7040 - val_precision_7: 0.7460 - val_recall_7: 0.6396\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load MobileNetV2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(128,128,3), \n",
    "                                               include_top=False, \n",
    "                                               weights=\"imagenet\")\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except for the last 10\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model with custom layers\n",
    "model_6 = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with a smaller learning rate\n",
    "model_6.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=[\n",
    "                    'mae', \n",
    "                    'accuracy', \n",
    "                    tf.keras.metrics.Precision(), \n",
    "                    tf.keras.metrics.Recall()\n",
    "                ])\n",
    "\n",
    "# Continue training\n",
    "history_6 = model_6.fit(train_generator, \n",
    "                             epochs=15, \n",
    "                             validation_data=validation_generator, \n",
    "                             class_weight=class_weight_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fbf2f-5abe-4942-8752-df46595c2901",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3ef54b9-5c9e-4459-9284-c32948d8e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 3s 130ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict using the validation generator\n",
    "validation_generator.reset() # To ensure it's starting from the beginning\n",
    "predictions = model_6.predict(validation_generator, steps=len(validation_generator))\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the true labels\n",
    "true_classes = validation_generator.classes\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd725513-7498-4fd3-9011-a5fe174c0ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.ylabel('True Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0457797-695b-4885-8134-5f7912dca627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    glioma_tumor       0.73      0.85      0.79       185\n",
      "meningioma_tumor       0.66      0.65      0.65       187\n",
      "        no_tumor       0.67      0.52      0.58       100\n",
      " pituitary_tumor       0.75      0.71      0.73       180\n",
      "\n",
      "        accuracy                           0.71       652\n",
      "       macro avg       0.70      0.68      0.69       652\n",
      "    weighted avg       0.70      0.71      0.70       652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_classes, predicted_classes, target_names=class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "070d8e48-410f-4c7b-bf10-81a3748712d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'glioma_tumor': 0, 'meningioma_tumor': 1, 'no_tumor': 2, 'pituitary_tumor': 3}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
